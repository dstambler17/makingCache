Daniel Stambler and Kevin Sherman
dstambl2@jhu.edu ksherma6@jhu.edu

HW6 Part 1 Observations and Answers
When we run the matrix file with 0 optimization and with all default parameters, we get 167,916,067 I refs and 787 misses, and 83,941,235 D refs and 1,050,434 D1 misses. We also get 1,051,221 LL refs, and 1,051,255 LLd misses.
The average time was 0.04 seconds with a 95%.

    Now we use Optimization 1:
    Performance is slightly higher a average time is now 0.037 seconds.
      However, there are more 80%s here.
      There is no notable change in number of misses or refs
    Now we use Optimization 2
      Performance is about the same as with optimization 0 (about 0.04 sec).
      However, there are more 80% than with other levels.
      The number of misses and refs is still unchanged.
    Now we use Optimization 3
      Performance is about the same as with optimization 0 (about 0.04 sec).
      Here there are about an equal number of 75% as 80% in the prev optimization.
    Now we use Optimization 4
      Performance is about 0.037 sec).
      Here there are more of 75% than previous optimizations.
Now we swap i and j
    Overall, for all optimizations, when i and j were swapped, the program ran much slower
    and the miss rate was much higher.

    Using Optimization 0
      much slower
      time percentage much higher - always close to 100%
      20% miss rate
    Using Optimization 1
      99% d1 miss rate
      time percentages close to 100%
      slow
    Using Optimization
      same result as above
    Using Optimization 3
      fast
      99.1% miss rate
    Using Optimization 4
      Performance and miss rate were the same as with using optimization 3.

Reverting i and j back, we now experiment the SIZE parameter
We noticed that there is almost no change in performance with smaller size parameters.
For example, when we change SIZE to 1024, the speed improves slightly. However, changing to 4096 see a notable change in performance. This is because large caches work much slower

Conclusions
Overall, we conclude that Optimizing when i and j are in the right order will not have as
much an effect as when you optimize when i and j were swapped.
When you swap i and j, you violate the locality rule of the cache and thus, accessing different parts of the cache becomes much smaller, leaving more room for optimization.
When we experiment with the size parameter, we also find that optimizing helps for the caches with the larger parameters.

-------------------------------------------------------------------------------------------
HW6 Part 2
To run the program (which is in python) type
python3 csim.py (followed by command line arguments)

For example:
python3 csim.py 256 4 16 1 0 1 gcc.trace

Brief Explanation
We created a Cache object and a Block Object for this program
Most of our program was handled in the cache object class file.
Each block object was made up of a bit that could be dirty, a tag, a counter
for FIFO, and a counter for least recently used.
